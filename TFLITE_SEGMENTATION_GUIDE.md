# TensorFlow Lite - Ghid Complet pentru Segmentarea CartonaÈ™elor Hot Wheels

## ğŸ“‹ Cuprins
1. [Prezentare GeneralÄƒ](#prezentare-generalÄƒ)
2. [Arhitectura Modelului](#arhitectura-modelului)
3. [Crearea Dataset-ului](#crearea-dataset-ului)
4. [Anotarea Imaginilor](#anotarea-imaginilor)
5. [Antrenarea Modelului](#antrenarea-modelului)
6. [Conversia la TFLite](#conversia-la-tflite)
7. [Integrarea Ã®n AplicaÈ›ie](#integrarea-Ã®n-aplicaÈ›ie)
8. [Resurse È™i Tools](#resurse-È™i-tools)

---

## ğŸ¯ Prezentare GeneralÄƒ

### Ce este TensorFlow Lite?
TensorFlow Lite este o versiune optimizatÄƒ a TensorFlow pentru dispozitive mobile È™i embedded. Este perfect pentru aplicaÈ›ii Android care necesitÄƒ inferenÈ›Äƒ ML Ã®n timp real.

### De ce TFLite pentru segmentarea cartonaÈ™elor?
- âœ… **PerformanÈ›Äƒ**: RuleazÄƒ direct pe device (nu necesitÄƒ server)
- âœ… **VitezÄƒ**: InferenÈ›Äƒ rapidÄƒ (< 1 secundÄƒ)
- âœ… **MÄƒrime**: Model optimizat (1-8 MB Ã®n APK)
- âœ… **Precizie**: Poate detecta forme complexe (nu doar dreptunghiuri)
- âœ… **Robust**: FuncÈ›ioneazÄƒ cu diferite unghiuri, distanÈ›e, iluminÄƒri

### DiferenÈ›a faÈ›Äƒ de Template Matching
- **Template Matching** (actual): CautÄƒ o formÄƒ fixÄƒ Ã®n imagine â†’ limitat la forme simple
- **TFLite Segmentation**: ÃnvaÈ›Äƒ sÄƒ recunoascÄƒ cartonaÈ™ul Ã®n orice poziÈ›ie/ipostazÄƒ â†’ mult mai robust

---

## ğŸ—ï¸ Arhitectura Modelului

### OpÈ›iuni de Modele

#### 1. **UNet** (Recomandat pentru Ã®nceput)
- **Avantaje**: Simplu, eficient, bun pentru segmentare precisÄƒ
- **MÄƒrime**: ~2-4 MB
- **VitezÄƒ**: ~200-500ms pe device
- **Precizie**: Foarte bunÄƒ pentru obiecte cu forme complexe

#### 2. **DeepLabv3-MobileNet** (AlternativÄƒ)
- **Avantaje**: Pre-antrenat, mai rapid
- **MÄƒrime**: ~1-2 MB
- **VitezÄƒ**: ~100-300ms
- **Precizie**: BunÄƒ, dar poate fi mai puÈ›in precisÄƒ la margini

#### 3. **DeepLabv3+ MobileNet** (Cel mai rapid)
- **Avantaje**: Cel mai optimizat pentru mobile
- **MÄƒrime**: ~1-2 MB
- **VitezÄƒ**: ~50-200ms
- **Precizie**: AcceptabilÄƒ pentru majoritatea cazurilor

### Recomandare FinalÄƒ
**Ãncepe cu UNet** - este cel mai simplu de antrenat È™i oferÄƒ cea mai bunÄƒ precizie pentru forme complexe precum cartonaÈ™ele Hot Wheels.

---

## ğŸ“¸ Crearea Dataset-ului

### CÃ¢te Imagini Ai Nevoie?

#### Minimum Viable:
- **200-300 imagini** pentru un model funcÈ›ional
- **500+ imagini** pentru un model robust È™i precis
- **1000+ imagini** pentru producÈ›ie (opÈ›ional)

### DistribuÈ›ia pe Tipuri de CartonaÈ™e

Ai menÈ›ionat cÄƒ ai **7-8 tipuri de cartonaÈ™e**:
- CartonaÈ™ scurt (108x108)
- CartonaÈ™ lung (108x165)
- Premium
- Mare/Large
- È˜i Ã®ncÄƒ 2-3 tipuri

**Recomandare**: **50-70 imagini per tip** = **350-560 imagini total**

### Varietatea Imaginilor

Fiecare tip de cartonaÈ™ trebuie fotografiat Ã®n:
- âœ… **Diferite distanÈ›e**: aproape, departe, mediu
- âœ… **Diferite unghiuri**: frontal, uÈ™or Ã®nclinat stÃ¢nga/dreapta, sus/jos
- âœ… **Diferite iluminÄƒri**: naturalÄƒ, artificialÄƒ, umbrÄƒ parÈ›ialÄƒ
- âœ… **Diferite fundaluri**: alb, colorat, texturat
- âœ… **Diferite poziÈ›ii Ã®n cadru**: centru, margini, parÈ›ial Ã®n afara cadrului

### Structura Dataset-ului

```
dataset/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ 1.jpg
â”‚   â”œâ”€â”€ 2.jpg
â”‚   â”œâ”€â”€ 3.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ masks/
â”‚   â”œâ”€â”€ 1.png
â”‚   â”œâ”€â”€ 2.png
â”‚   â”œâ”€â”€ 3.png
â”‚   â””â”€â”€ ...
â””â”€â”€ annotations.json (opÈ›ional - pentru metadata)
```

**ConvenÈ›ie de nume**: 
- Imagine: `1.jpg`, `2.jpg`, etc.
- MascÄƒ: `1.png`, `2.png`, etc. (acelaÈ™i numÄƒr!)

---

## ğŸ¨ Anotarea Imaginilor

### Ce este o MascÄƒ?

O **mascÄƒ** este o imagine PNG cu:
- **Fundal negru (0,0,0)** = zonele care NU sunt cartonaÈ™
- **CartonaÈ™ alb (255,255,255)** = zona care ESTE cartonaÈ™
- **OpÈ›ional**: Zone gri pentru margini fuzzy (anti-aliasing)

### Cum SÄƒ Faci AnotÄƒrile?

#### OpÈ›iunea 1: **LabelMe** (Recomandat - Gratuit)
- **Download**: https://github.com/wkentaro/labelme
- **Instalare**: `pip install labelme`
- **Utilizare**:
  1. Deschizi imaginea
  2. Desenezi conturul cartonaÈ™ului cu poligon
  3. Salvezi ca JSON
  4. Export ca PNG mask

**Avantaje**: 
- Gratuit È™i open-source
- SuportÄƒ forme complexe (poligoane, nu doar dreptunghiuri)
- Export direct la PNG masks
- Batch processing

#### OpÈ›iunea 2: **CVAT** (Pentru echipe)
- **Website**: https://cvat.org/
- **Avantaje**: Colaborare, workflow profesional
- **Dezavantaje**: Mai complex, necesitÄƒ server

#### OpÈ›iunea 3: **Photoshop/GIMP** (Manual)
- Deschizi imaginea
- Selectezi cartonaÈ™ul cu Pen Tool (pentru forme complexe)
- Creezi mascÄƒ (Select â†’ Save Selection)
- Export ca PNG (alb pe negru)

### Detalii Importante pentru AnotÄƒri

#### Forma CartonaÈ™ului
CartonaÈ™ele Hot Wheels **NU sunt dreptunghiuri perfecte**:
- Au colÈ›uri tÄƒiate
- Au decupaje Ã®n formÄƒ de "umeras"
- Margini rotunjite
- Forme complexe

**SoluÈ›ie**: FoloseÈ™te **poligoane** Ã®n LabelMe pentru a urmÄƒri exact conturul!

#### Precizia AnotÄƒrilor
- âœ… **Foarte important**: AnotÄƒrile trebuie sÄƒ fie **precise**
- âœ… UrmeazÄƒ exact marginea cartonaÈ™ului (nu mai mult, nu mai puÈ›in)
- âœ… Pentru margini fuzzy, foloseÈ™te gri (anti-aliasing)
- âŒ **EvitÄƒ**: AnotÄƒri aproximative sau "aproape bune"

#### Batch Anotare
DupÄƒ ce ai 50-100 imagini anotate, poÈ›i folosi:
- **Data augmentation** (rotaÈ›ii, flip-uri, brightness) pentru a multiplica dataset-ul
- **Semi-supervised learning** (opÈ›ional, avansat)

---

## ğŸš€ Antrenarea Modelului

### Setup IniÈ›ial

#### 1. InstaleazÄƒ TensorFlow

**Pentru RTX 5070 (CUDA 12.x):**

**OpÈ›iunea 1: pip (recomandat)**
```bash
# VerificÄƒ mai Ã®ntÃ¢i GPU-ul:
nvidia-smi

# InstaleazÄƒ TensorFlow cu CUDA 12.x support:
pip install tensorflow[and-cuda]

# SAU versiunea specificÄƒ:
pip install tensorflow==2.15.0
```

**OpÈ›iunea 2: conda (mai simplu pentru CUDA/cuDNN)**
```bash
conda create -n tf-gpu python=3.10
conda activate tf-gpu
conda install -c conda-forge tensorflow-gpu cudatoolkit=12.2 cudnn=8.9
```

**VerificÄƒ instalarea:**
```python
import tensorflow as tf
print("TensorFlow version:", tf.__version__)
print("GPU Available:", len(tf.config.list_physical_devices('GPU')) > 0)
if tf.config.list_physical_devices('GPU'):
    print("GPU Name:", tf.config.list_physical_devices('GPU')[0].name)
    # Ar trebui sÄƒ vezi: /physical_device:GPU:0
```

**DacÄƒ NU vezi GPU-ul:**
1. VerificÄƒ cÄƒ ai instalat CUDA 12.x
2. VerificÄƒ cÄƒ ai instalat cuDNN 8.9.x
3. ReinstaleazÄƒ TensorFlow: `pip uninstall tensorflow tensorflow-gpu && pip install tensorflow[and-cuda]`

#### 2. InstaleazÄƒ LibrÄƒrii Suplimentare
```bash
pip install numpy
pip install opencv-python
pip install matplotlib
pip install pillow
pip install scikit-learn
```

### Structura Codului de Antrenare

#### Pasul 1: ÃncarcÄƒ Dataset-ul
```python
import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split

def load_dataset(images_dir, masks_dir):
    images = []
    masks = []
    
    # AsigurÄƒ-te cÄƒ numele fiÈ™ierelor se potrivesc
    for filename in os.listdir(images_dir):
        if filename.endswith('.jpg'):
            img_path = os.path.join(images_dir, filename)
            mask_path = os.path.join(masks_dir, filename.replace('.jpg', '.png'))
            
            if os.path.exists(mask_path):
                img = cv2.imread(img_path)
                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
                
                # RedimensioneazÄƒ la dimensiune fixÄƒ (ex: 256x256 sau 512x512)
                img = cv2.resize(img, (256, 256))
                mask = cv2.resize(mask, (256, 256))
                
                # NormalizeazÄƒ
                img = img.astype(np.float32) / 255.0
                mask = (mask > 127).astype(np.float32)  # BinarizeazÄƒ masca
                
                images.append(img)
                masks.append(mask)
    
    return np.array(images), np.array(masks)

# ÃncarcÄƒ datele
images, masks = load_dataset('dataset/images', 'dataset/masks')
```

#### Pasul 2: Split Train/Validation
```python
X_train, X_val, y_train, y_val = train_test_split(
    images, masks, test_size=0.2, random_state=42
)
```

#### Pasul 3: Data Augmentation
```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Augmentare pentru imagini
img_datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    brightness_range=[0.8, 1.2]
)

# Augmentare pentru mÄƒÈ™ti (doar transformÄƒri geometrice, NU brightness!)
mask_datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True
)
```

#### Pasul 4: ConstruieÈ™te Modelul UNet
```python
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate

def build_unet(input_size=(256, 256, 3)):
    inputs = Input(input_size)
    
    # Encoder (Downsampling)
    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)
    p1 = MaxPooling2D((2, 2))(c1)
    
    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)
    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)
    p2 = MaxPooling2D((2, 2))(c2)
    
    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)
    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)
    p3 = MaxPooling2D((2, 2))(c3)
    
    # Bottleneck
    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)
    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)
    
    # Decoder (Upsampling)
    u5 = UpSampling2D((2, 2))(c4)
    u5 = Concatenate()([u5, c3])
    c5 = Conv2D(256, (3, 3), activation='relu', padding='same')(u5)
    c5 = Conv2D(256, (3, 3), activation='relu', padding='same')(c5)
    
    u6 = UpSampling2D((2, 2))(c5)
    u6 = Concatenate()([u6, c2])
    c6 = Conv2D(128, (3, 3), activation='relu', padding='same')(u6)
    c6 = Conv2D(128, (3, 3), activation='relu', padding='same')(c6)
    
    u7 = UpSampling2D((2, 2))(c6)
    u7 = Concatenate()([u7, c1])
    c7 = Conv2D(64, (3, 3), activation='relu', padding='same')(u7)
    c7 = Conv2D(64, (3, 3), activation='relu', padding='same')(c7)
    
    # Output layer
    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c7)
    
    model = Model(inputs=inputs, outputs=outputs)
    return model

model = build_unet()
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
```

#### Pasul 5: Antrenare

**OptimizÄƒri pentru RTX 5070 + 32GB RAM:**

```python
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import tensorflow as tf

# ConfigureazÄƒ GPU memory growth (pentru RTX 5070)
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# Callbacks
callbacks = [
    ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', verbose=1),
    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1)
]

# Antrenare cu batch_size optimizat pentru RTX 5070
history = model.fit(
    X_train, y_train,
    batch_size=16,  # âœ… Cu 32GB RAM È™i RTX 5070, poÈ›i folosi 16-32
    epochs=50,      # âœ… Cu RTX 5070: ~30-60 min pentru 500 imagini
    validation_data=(X_val, y_val),
    callbacks=callbacks,
    verbose=1
)

# DacÄƒ ai erori OOM (Out of Memory), reduce batch_size la 8 sau 4
```

**Tips pentru RTX 5070:**
- âœ… PoÈ›i folosi `batch_size=16` sau chiar `32` (cu 32GB RAM)
- âœ… Mixed precision training (opÈ›ional, pentru vitezÄƒ suplimentarÄƒ):
```python
# AdaugÄƒ la Ã®nceputul scriptului
from tensorflow.keras.mixed_precision import set_global_policy
set_global_policy('mixed_float16')  # Accelerare ~1.5-2x
```

### Timp de Antrenare

#### Pentru RTX 5070 + Ryzen 7800X3D + 32GB RAM:
- **500 imagini, UNet, batch_size=16**: **30-60 minute** âš¡
- **1000 imagini, UNet, batch_size=16**: **1-2 ore**
- **Cu mixed precision**: **20-40 minute** pentru 500 imagini

#### ComparaÈ›ie:
- **CPU (chiar È™i Ryzen 7800X3D)**: 4-6 ore pentru 500 imagini
- **RTX 5070**: **30-60 minute** pentru 500 imagini (10-12x mai rapid!)
- **Google Colab (GPU T4)**: 1-2 ore pentru 500 imagini

**Recomandare**: 
- âœ… **Cu RTX 5070** â†’ AntreneazÄƒ **local** (mult mai rapid È™i convenabil!)
- âŒ Nu mai ai nevoie de Colab - calculatorul tÄƒu e mai rapid!

### Setup pentru Calculator Rapid (cu GPU)

#### ConfiguraÈ›ie RecomandatÄƒ: RTX 5070 + Ryzen 7800X3D + 32GB RAM

AceastÄƒ configuraÈ›ie este **excelentÄƒ** pentru antrenare! Vei putea antrena local foarte rapid.

#### Pasul 1: InstaleazÄƒ CUDA È™i cuDNN

**Pentru RTX 5070 (Ada Lovelace architecture):**
- **CUDA 12.x** (recomandat: CUDA 12.2 sau mai nou)
- **cuDNN 8.9.x** sau mai nou

**Download:**
1. CUDA: https://developer.nvidia.com/cuda-downloads
2. cuDNN: https://developer.nvidia.com/cudnn (necesitÄƒ cont NVIDIA gratuit)

**Sau foloseÈ™te conda (mai simplu):**
```bash
conda install -c conda-forge cudatoolkit=12.2 cudnn=8.9
```

#### Pasul 2: InstaleazÄƒ TensorFlow cu GPU Support

```bash
# Pentru RTX 5070 (CUDA 12.x):
pip install tensorflow[and-cuda]

# SAU dacÄƒ foloseÈ™ti conda:
conda install -c conda-forge tensorflow-gpu
```

#### Pasul 3: VerificÄƒ Setup-ul

```bash
# VerificÄƒ GPU-ul
nvidia-smi
```

Ar trebui sÄƒ vezi RTX 5070 listat.

```python
# VerificÄƒ Ã®n Python
import tensorflow as tf
print("TensorFlow version:", tf.__version__)
print("GPU Available:", len(tf.config.list_physical_devices('GPU')) > 0)
if tf.config.list_physical_devices('GPU'):
    print("GPU Name:", tf.config.list_physical_devices('GPU')[0].name)
    # ConfigureazÄƒ memory growth pentru a evita OOM
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print("âœ… GPU memory growth enabled")
        except RuntimeError as e:
            print(e)
```

#### EstimÄƒri de Timp pentru RTX 5070:

- **500 imagini, UNet, batch_size=16**: **30-60 minute** (foarte rapid!)
- **1000 imagini, UNet, batch_size=16**: **1-2 ore**
- **Cu data augmentation**: +20-30% timp

**Avantaje:**
- âœ… Antrenare localÄƒ (nu depinzi de Colab)
- âœ… Control complet asupra procesului
- âœ… PoÈ›i rula multiple experimente rapid
- âœ… 32GB RAM = poÈ›i folosi batch_size mai mare (16-32)

---

## ğŸ“¦ Conversia la TFLite

### Pasul 1: SalveazÄƒ Modelul
```python
model.save('unet_model.h5')
```

### Pasul 2: Conversie la TFLite
```python
import tensorflow as tf

# ÃncarcÄƒ modelul
model = tf.keras.models.load_model('best_model.h5')

# ConverteÈ™te la TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# OptimizÄƒri (opÈ›ional, dar recomandat)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# Quantizare (reduce mÄƒrimea modelului)
converter.target_spec.supported_types = [tf.float16]  # sau tf.int8 pentru mai mic

# ConverteÈ™te
tflite_model = converter.convert()

# SalveazÄƒ
with open('card_segmentation.tflite', 'wb') as f:
    f.write(tflite_model)
```

### MÄƒrime Model Final
- **Float32**: ~8-12 MB
- **Float16**: ~4-6 MB (recomandat)
- **Int8**: ~2-3 MB (poate pierde precizie)

---

## ğŸ“± Integrarea Ã®n AplicaÈ›ie Android

### Pasul 1: AdaugÄƒ TFLite Ã®n Proiect

#### `build.gradle` (Module: app)
```gradle
dependencies {
    // TensorFlow Lite
    implementation 'org.tensorflow:tensorflow-lite:2.14.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu:2.14.0'  // OpÈ›ional - pentru GPU
    
    // Support pentru imagini
    implementation 'org.tensorflow:tensorflow-lite-support:0.4.4'
}
```

#### CopiazÄƒ Modelul
CopiazÄƒ `card_segmentation.tflite` Ã®n:
```
app/src/main/assets/models/card_segmentation.tflite
```

### Pasul 2: CreeazÄƒ TFLite Manager

#### `TFLiteSegmentationManager.kt`
```kotlin
package com.example.hotwheelscollectors.domain.manager

import android.content.Context
import android.graphics.Bitmap
import org.tensorflow.lite.Interpreter
import java.io.FileInputStream
import java.io.FileOutputStream
import java.nio.ByteBuffer
import java.nio.ByteOrder
import java.nio.MappedByteBuffer
import java.nio.channels.FileChannel

class TFLiteSegmentationManager(private val context: Context) {
    
    private var interpreter: Interpreter? = null
    private val INPUT_SIZE = 256  // Dimensiunea la care ai antrenat modelul
    private val PIXEL_SIZE = 3    // RGB
    
    init {
        loadModel()
    }
    
    private fun loadModel() {
        try {
            val modelFile = loadModelFile("card_segmentation.tflite")
            interpreter = Interpreter(modelFile)
            Timber.d("âœ… TFLite model loaded successfully")
        } catch (e: Exception) {
            Timber.e(e, "âŒ Failed to load TFLite model")
        }
    }
    
    private fun loadModelFile(modelPath: String): MappedByteBuffer {
        val assetFileDescriptor = context.assets.openFd(modelPath)
        val fileInputStream = FileInputStream(assetFileDescriptor.fileDescriptor)
        val fileChannel = fileInputStream.channel
        val startOffset = assetFileDescriptor.startOffset
        val declaredLength = assetFileDescriptor.declaredLength
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
    }
    
    /**
     * ProceseazÄƒ o imagine È™i returneazÄƒ masca segmentatÄƒ
     */
    fun segmentCard(source: Bitmap): Bitmap? {
        val interpreter = this.interpreter ?: return null
        
        // 1. RedimensioneazÄƒ imaginea la INPUT_SIZE
        val resizedBitmap = Bitmap.createScaledBitmap(
            source,
            INPUT_SIZE,
            INPUT_SIZE,
            true
        )
        
        // 2. ConverteÈ™te Bitmap la ByteBuffer
        val inputBuffer = bitmapToByteBuffer(resizedBitmap)
        
        // 3. PregÄƒteÈ™te output buffer
        val outputShape = interpreter.getOutputTensor(0).shape()
        val outputBuffer = ByteBuffer.allocateDirect(
            outputShape[1] * outputShape[2] * outputShape[3] * 4  // Float32 = 4 bytes
        ).order(ByteOrder.nativeOrder())
        
        // 4. RuleazÄƒ inferenÈ›a
        interpreter.run(inputBuffer, outputBuffer)
        
        // 5. ConverteÈ™te output la Bitmap (mascÄƒ)
        val maskBitmap = outputBufferToBitmap(outputBuffer, outputShape)
        
        // 6. RedimensioneazÄƒ masca la dimensiunea originalÄƒ
        return Bitmap.createScaledBitmap(
            maskBitmap,
            source.width,
            source.height,
            true
        )
    }
    
    private fun bitmapToByteBuffer(bitmap: Bitmap): ByteBuffer {
        val byteBuffer = ByteBuffer.allocateDirect(
            INPUT_SIZE * INPUT_SIZE * PIXEL_SIZE * 4  // Float32
        ).order(ByteOrder.nativeOrder())
        
        val pixels = IntArray(INPUT_SIZE * INPUT_SIZE)
        bitmap.getPixels(pixels, 0, INPUT_SIZE, 0, 0, INPUT_SIZE, INPUT_SIZE)
        
        var pixel = 0
        for (i in 0 until INPUT_SIZE) {
            for (j in 0 until INPUT_SIZE) {
                val pixelValue = pixels[pixel++]
                
                // NormalizeazÄƒ la [0, 1]
                byteBuffer.putFloat(((pixelValue shr 16) and 0xFF) / 255.0f)  // R
                byteBuffer.putFloat(((pixelValue shr 8) and 0xFF) / 255.0f)   // G
                byteBuffer.putFloat((pixelValue and 0xFF) / 255.0f)            // B
            }
        }
        
        return byteBuffer
    }
    
    private fun outputBufferToBitmap(
        buffer: ByteBuffer,
        shape: IntArray
    ): Bitmap {
        buffer.rewind()
        val width = shape[1]
        val height = shape[2]
        val maskBitmap = Bitmap.createBitmap(width, height, Bitmap.Config.ARGB_8888)
        
        val pixels = IntArray(width * height)
        for (i in 0 until height) {
            for (j in 0 until width) {
                val value = buffer.float
                // ConverteÈ™te probabilitatea [0,1] la [0,255]
                val grayValue = (value * 255).toInt().coerceIn(0, 255)
                pixels[i * width + j] = android.graphics.Color.rgb(
                    grayValue, grayValue, grayValue
                )
            }
        }
        maskBitmap.setPixels(pixels, 0, width, 0, 0, width, height)
        return maskBitmap
    }
    
    /**
     * AplicÄƒ masca pe imaginea originalÄƒ È™i extrage cartonaÈ™ul
     */
    fun extractCardWithMask(source: Bitmap, mask: Bitmap): Bitmap {
        val result = Bitmap.createBitmap(
            source.width,
            source.height,
            Bitmap.Config.ARGB_8888
        )
        val canvas = android.graphics.Canvas(result)
        
        // Fundal alb
        canvas.drawColor(android.graphics.Color.WHITE)
        
        // DeseneazÄƒ doar zonele din mascÄƒ
        val paint = android.graphics.Paint()
        val srcRect = android.graphics.Rect(0, 0, source.width, source.height)
        val dstRect = android.graphics.Rect(0, 0, source.width, source.height)
        
        // CreeazÄƒ un BitmapShader pentru a aplica masca
        val shader = android.graphics.BitmapShader(
            source,
            android.graphics.Shader.TileMode.CLAMP,
            android.graphics.Shader.TileMode.CLAMP
        )
        paint.shader = shader
        
        // AplicÄƒ masca ca alpha mask
        val maskPaint = android.graphics.Paint()
        maskPaint.xfermode = android.graphics.PorterDuffXfermode(
            android.graphics.PorterDuff.Mode.DST_IN
        )
        
        canvas.drawBitmap(source, srcRect, dstRect, paint)
        canvas.drawBitmap(mask, srcRect, dstRect, maskPaint)
        
        return result
    }
    
    fun release() {
        interpreter?.close()
        interpreter = null
    }
}
```

### Pasul 3: IntegreazÄƒ Ã®n CameraManager

#### ModificÄƒ `CameraManager.kt`
```kotlin
class CameraManager @Inject constructor(
    @ApplicationContext private val context: Context
) {
    private val tfliteManager = TFLiteSegmentationManager(context)
    
    /**
     * ProceseazÄƒ poza folosind TFLite Ã®n loc de template matching
     */
    suspend fun processPhotoWithTFLite(photoUri: Uri): Bitmap? = withContext(Dispatchers.Default) {
        try {
            // 1. ÃncarcÄƒ imaginea
            val sourceBitmap = loadBitmapFromUri(photoUri) ?: return@withContext null
            
            // 2. ObÈ›ine masca din TFLite
            val mask = tfliteManager.segmentCard(sourceBitmap)
                ?: return@withContext null
            
            // 3. Extrage cartonaÈ™ul folosind masca
            val extractedCard = tfliteManager.extractCardWithMask(sourceBitmap, mask)
            
            // 4. ReturneazÄƒ rezultatul
            extractedCard
        } catch (e: Exception) {
            Timber.e(e, "Failed to process photo with TFLite")
            null
        }
    }
}
```

---

## ğŸ› ï¸ Resurse È™i Tools

### Tools pentru Anotare
1. **LabelMe**: https://github.com/wkentaro/labelme
2. **CVAT**: https://cvat.org/
3. **Roboflow**: https://roboflow.com/ (cloud-based, opÈ›ional)

### Resurse de ÃnvÄƒÈ›are
1. **TensorFlow Lite Tutorial**: https://www.tensorflow.org/lite
2. **UNet Paper**: https://arxiv.org/abs/1505.04597
3. **Image Segmentation Guide**: https://www.tensorflow.org/tutorials/images/segmentation

### Google Colab Templates
- CautÄƒ "UNet TensorFlow Colab" pentru template-uri gata de folosit
- Sau foloseÈ™te acest template: https://github.com/zhixuhao/unet

### Dataset-uri Publice (pentru referinÈ›Äƒ)
- **COCO Dataset**: https://cocodataset.org/ (pentru a vedea cum aratÄƒ anotÄƒrile profesionale)
- **Cityscapes**: https://www.cityscapes-dataset.com/ (pentru segmentare urbanÄƒ)

---

## â±ï¸ Timeline Estimativ

### Faza 1: Dataset (1-2 sÄƒptÄƒmÃ¢ni)
- Fotografiere: 2-3 zile (350-560 imagini)
- Anotare: 5-10 zile (50-100 imagini/zi cu LabelMe)

### Faza 2: Antrenare (1 zi - cu RTX 5070!)
- Setup CUDA/TensorFlow: 2-3 ore
- Antrenare: **30-60 minute** pentru 500 imagini (foarte rapid cu RTX 5070!)
- Testare È™i optimizare: 2-3 ore

### Faza 3: Integrare (2-3 zile)
- Conversie TFLite: 0.5 zi
- Integrare Ã®n app: 1-2 zile
- Testare È™i optimizare: 1 zi

**Total**: ~2-3 sÄƒptÄƒmÃ¢ni pentru un model funcÈ›ional

---

## âœ… Checklist Final

### Ãnainte de a Ãncepe
- [ ] InstaleazÄƒ LabelMe
- [ ] PregÄƒteÈ™te camera pentru fotografiere
- [ ] CreeazÄƒ structura de foldere pentru dataset

### Ãn Timpul AnotÄƒrii
- [ ] AsigurÄƒ-te cÄƒ numele fiÈ™ierelor se potrivesc (1.jpg â†’ 1.png)
- [ ] VerificÄƒ calitatea anotÄƒrilor (precizie la margini)
- [ ] TesteazÄƒ pe 10-20 imagini Ã®nainte de a continua

### DupÄƒ Antrenare
- [ ] TesteazÄƒ modelul pe imagini noi (nu din dataset)
- [ ] VerificÄƒ precizia (mÄƒsurÄƒ cu IoU - Intersection over Union)
- [ ] OptimizeazÄƒ mÄƒrimea modelului (quantizare)

### Ãn AplicaÈ›ie
- [ ] TesteazÄƒ pe device-uri reale (nu doar emulator)
- [ ] MÄƒsoarÄƒ timpul de inferenÈ›Äƒ
- [ ] ComparÄƒ cu template matching (vechi) pentru a vedea Ã®mbunÄƒtÄƒÈ›irea

---

## ğŸ¯ Rezumat Executiv

1. **FotografiazÄƒ 350-560 imagini** (50-70 per tip de cartonaÈ™)
2. **AnoteazÄƒ cu LabelMe** (poligoane precise pe conturul cartonaÈ™ului)
3. **AntreneazÄƒ UNet Ã®n Python** (local pe calculator rapid sau Google Colab)
4. **ConverteÈ™te la TFLite** (Float16 pentru balanÈ›Äƒ mÄƒrime/precizie)
5. **IntegreazÄƒ Ã®n app cu Kotlin** (foloseÈ™te `TFLiteSegmentationManager`)
6. **TesteazÄƒ È™i optimizeazÄƒ** (mÄƒsoarÄƒ precizia È™i viteza)

### âš ï¸ Important: Python vs Kotlin

- **Python** = Pentru **antrenare** (obligatoriu, TensorFlow/Keras ruleazÄƒ Ã®n Python)
- **Kotlin** = Pentru **inferenÈ›Äƒ Ã®n app** (TensorFlow Lite ruleazÄƒ Ã®n Kotlin/Android)

Nu poÈ›i antrena modelul Ã®n Kotlin - doar Ã®n Python. Dar rularea modelului (inferenÈ›a) se face Ã®n Kotlin Ã®n aplicaÈ›ia ta Android.

**Rezultat Final**: Un model care detecteazÄƒ cartonaÈ™ele Hot Wheels Ã®n orice poziÈ›ie, unghi sau distanÈ›Äƒ, mult mai robust decÃ¢t template matching!

---

## ğŸ“ Suport

DacÄƒ Ã®ntÃ¢mpini probleme:
1. VerificÄƒ cÄƒ anotÄƒrile sunt corecte (mÄƒÈ™ti PNG albe pe negru)
2. VerificÄƒ cÄƒ dimensiunile imaginilor sunt consistente
3. VerificÄƒ cÄƒ modelul se Ã®ncarcÄƒ corect Ã®n app (check logs)
4. TesteazÄƒ modelul separat (Ã®nainte de integrare) cu imagini de test

**Succes! ğŸš€**

